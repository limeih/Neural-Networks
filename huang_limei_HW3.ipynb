{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "huang_limei_HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fDEQjuu_iJ3g"
      },
      "source": [
        "# HW 03 - Create a word2vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bRrPQL2diJ3j"
      },
      "source": [
        "<font color=blue size=4>\n",
        "Before you submit this assignment, please carefully read these submission instructions. You must name this .ipynb file:\n",
        "<br><br>\n",
        "yourlastname_yourfirstname_HW3.ipynb\n",
        "<br><br>\n",
        "You must turn in this assignment by uploading the \n",
        ".ipynb file to the assignment on questrom tools. You will also need to print out a hard copy of this notebook (File->Print from colab) with the output from running all the code cells, and hand it in on the class following the due date. Do not email me the file.\n",
        "<br><br>\n",
        "Points will be deducted for improper submission!\n",
        "</font>\n",
        "\n",
        "For this assignment, we will use Pytorch to create a word2vec model that infers numerical vectors for words that capture their meaning. Word2vec was first introduced in 2013 by Mikolov et al. at Google. Their paper can be found [here](https://arxiv.org/pdf/1301.3781.pdf), though you do not need to read and understand it in order to implement the model. It is a very popular machine learning model that has been implemented to capture the meaning of text for many real world cases. \n",
        "\n",
        "This [blog post](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/) is a great overview of word2vec. Please read it carefully before you create the word2vec model for this assignment. Specifically, you will build a \"Continuous Bag-of-Words Model (CBOW)\" word2vec model. CBOW predicts a focal (target) word from its context (the words surround it). The following Youtube videos also explain the concept of the CBOW model.\n",
        "- https://www.youtube.com/watch?v=UqRCEmrv1gQ\n",
        "- https://www.youtube.com/watch?v=gQddtTdmG_8 \n",
        "\n",
        "\n",
        "<img src=\"https://i2.wp.com/www.stokastik.in/wp-content/uploads/2017/04/Screen-Shot-2017-05-16-at-8.48.52-PM.png?w=596\">\n",
        "\n",
        "[CBOW structure from http://www.stokastik.in/understanding-word-vectors-and-word2vec/]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rayw7XfBiJ3u"
      },
      "source": [
        "Your task is to create a CBOW neural network model class called `CBOW`. `CBOW` has the structure shown above and the following properties:\n",
        "\n",
        "- `vocab_size` - Size of vocabulary($V$). Note that vocabulary is a set of unique words in a corpus (a bunch of text documents).\n",
        "- `embed_dim` - Dimension of the embedding vector\n",
        "- `window_size` - Size of window. If a focal word is at position $t$, then the CBOW model uses embedding vectors of words between ($t$-window_size) and ($t$+window_size) to predict the focal word\n",
        "- `hidden_dim` - Dimension of the hidden layer ($N$)\n",
        "\n",
        "`CBOW` consists of three layers:\n",
        "\n",
        "- `embedding` - An embedding layer that is initialized with `torch.nn.Embedding`\n",
        "- `fc1` - A linear transformation that connects the embedding layer to the hidden layer. `torch.nn.functional.relu` activation should be applied to the output of `fc1`.\n",
        "- `fc2` - A linear transformation that connects the activation of `fc1` to a tensor of length `vocab_size`. \n",
        "\n",
        "The training data (i.e., the features `X`, the labels `y`) that we will use to train the `CBOW` model will be:\n",
        "- `X` will be a tensor of length (2 * `window_size`) containing the indices of all words in the window before and after the focal word. \n",
        "- `y`, (the label that our model is trying to predict) should be a list containing the index of the focal word.\n",
        "\n",
        "Note that a single review in our data will produce multiple items of training data. For example, suppose a single review is:\n",
        "\n",
        " \"the food was not good at all\"\n",
        " \n",
        " If our `window_size` = 2, then this would generate the following (context, focal_word) training data tuples:\n",
        "\n",
        "```python\n",
        "(['the','food','not','good'], ['was']) # 'was' is the focal word\n",
        "(['food','was','good','at'], ['not']) # 'not' is the focal word\n",
        "(['was','not','at','all'],['good']) # 'good' is the focal word\n",
        "```\n",
        "\n",
        "However, we can't directly use these tuples to train our model. First we have to replace each word with a unique integer (its index) and then convert these to pytorch tensors. Note that, we will be using a special embedding layer (`torch.nn.Embedding`) which will convert these indexes to the one-hot vectors that are described in the videos.\n",
        "\n",
        "To get tensors from the original data, you will need to:\n",
        "\n",
        "- Create a list (or `set`) of all unique words in the cleaned text, called `vocab`.\n",
        "- Create a dictionary called `word_to_index` where the key is a word and the value is the index of a word (a unique number for each word). You will have to figure out how to create this dictionary from the cleaned dataset.\n",
        "- Write a function `make_cbow_data` that accepts a single review from cleaned_text as an input and outputs a **list of tuples** where:\n",
        " -  the first part of the tuple contains a tensor of the indices of words in the window before and after each focal word\n",
        " - the second part of the tuple is a tensor containing the index of the focal word.\n",
        " - The dtype of both tensors in the tuple should be `torch.long`.\n",
        " - You will have to figure out how to create multiple tuples of tensors from a single review (an item from `cleaned_text`) using loops \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P6uzFDXFiJ3v"
      },
      "source": [
        "We will use restaurant customer reviews data for this assignment.\n",
        "\n",
        "**Do not change the code block below**. Below is a function that cleans up the text of a review and returns a list of all the words in the review.\n",
        "\n",
        "You will use `cleaned_text`, which is defined below, to create a training dataset for your `CBOW` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SYTmVZStiJ3w",
        "colab": {}
      },
      "source": [
        "# DO NOT CHANGE THIS CODEBLOCK\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "def clean_text(text):    \n",
        "    x = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
        "    x = x.lower().split() # lower case and split by whitespace to differentiate words\n",
        "    return x\n",
        "\n",
        "example_text = pd.read_csv('https://raw.githubusercontent.com/dylanwalker/BA865/master/datasets/hw3.csv')\n",
        "cleaned_text = example_text.Review[:100].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIXEOEhiNU-5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "cb75fa64-1769-42d0-cf4a-0b4db2a6ac1f"
      },
      "source": [
        "# print(cleaned_text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0                             [wow, loved, this, place]\n",
            "1                                [crust, is, not, good]\n",
            "2     [not, tasty, and, the, texture, was, just, nasty]\n",
            "3     [stopped, by, during, the, late, may, bank, ho...\n",
            "4     [the, selection, on, the, menu, was, great, an...\n",
            "                            ...                        \n",
            "95                             [well, never, go, again]\n",
            "96                              [will, be, back, again]\n",
            "97                             [food, arrived, quickly]\n",
            "98                                 [it, was, not, good]\n",
            "99    [on, the, up, side, their, cafe, serves, reall...\n",
            "Name: Review, Length: 100, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZVonADFSPYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "fadff181-5f7e-41d2-d1ca-3428e41922d0"
      },
      "source": [
        "# tem = cleaned_text.iloc[:3].explode().unique().tolist()\n",
        "# tem.index(\"loved\")\n",
        "# index_tem = {i:tem.index(i) for i in tem}\n",
        "# index_tem\n",
        "# word_to_index = {word: i for i, word in enumerate(tem)}\n",
        "# word_to_index"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 9,\n",
              " 'crust': 4,\n",
              " 'good': 7,\n",
              " 'is': 5,\n",
              " 'just': 13,\n",
              " 'loved': 1,\n",
              " 'nasty': 14,\n",
              " 'not': 6,\n",
              " 'place': 3,\n",
              " 'tasty': 8,\n",
              " 'texture': 11,\n",
              " 'the': 10,\n",
              " 'this': 2,\n",
              " 'was': 12,\n",
              " 'wow': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HEl72ZkpTaQ-"
      },
      "source": [
        "## Create a CBOW Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpUT3gC6P3Dh",
        "colab_type": "text"
      },
      "source": [
        "The first step is to create `vocab` and `word_to_index` according to the instructions above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QQycg3vFhf8",
        "colab": {}
      },
      "source": [
        "#Create your vocab here\n",
        "vocab = cleaned_text.explode().unique().tolist()\n",
        "#Create your word_to_index dictionary here\n",
        "word_to_index = {word: vocab.index(word) for word in vocab}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HrIIKHqP_OM",
        "colab_type": "text"
      },
      "source": [
        "Now define your `make_cbow_data()` function. It should take `text` (text of a single review) as an argument, and `window_size` (the number of words to the left or right of the focal word) as an argument. It may also take other arguments, depending on how you define it. It should return a list of tuples as described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVScRx78hQQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define your make_cbow_data function here (it should accept arguments: text, window_size  -- and possibly other arguments depending on how you define it)\n",
        "import torch\n",
        "def make_cbow_data(review, window_size):\n",
        "  tup_list = []\n",
        "  for i in range(window_size, len(review) - window_size):\n",
        "    foc = torch.tensor([word_to_index[review[i]]], dtype = torch.long)\n",
        "    win_words = review[i-2:i] + review[i+1:i+3]\n",
        "    win_list = torch.tensor([word_to_index[word] for word in win_words], dtype = torch.long)\n",
        "    tem = (win_list, foc)\n",
        "    tup_list.append(tem)\n",
        "  return tup_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_Qs0eDuyFUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1c2858a7-50c7-409a-925a-87a92ec97aaa"
      },
      "source": [
        "# make_cbow_data([\"not\", \"tasty\", \"and\", \"the\", \"texture\", \"was\", \"just\", \"nasty\"], 3)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([ 8,  9, 11, 12]), tensor([10])),\n",
              " (tensor([ 9, 10, 12, 13]), tensor([11]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J09a8XTzE3L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3bb8f8f4-0f0b-4cb6-98be-970194197e59"
      },
      "source": [
        "# make_cbow_data(\"not tasty and the texture was just nasty\", 3)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([ 8,  9, 11, 12]), tensor([10])),\n",
              " (tensor([ 9, 10, 12, 13]), tensor([11]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6kWcdS4QZzM",
        "colab_type": "text"
      },
      "source": [
        "Now define your CBOW model class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUjiOY8t5P-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-sMXIE6heYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define your CBOW model here\n",
        "class CBOW(torch.nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, window_size, hidden_dim):\n",
        "    super(CBOW, self).__init__()\n",
        "    # embedding layer initialized with torch.nn.Embedding\n",
        "    self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "    # fc1: a linear transformation connects embedding layer and hidden layer\n",
        "    self.fc1 = torch.nn.Linear(2 * window_size * embed_dim, hidden_dim)\n",
        "    # relu activaton should be applied to the output of fc1\n",
        "    self.relu1 = torch.nn.ReLU()\n",
        "    # fc2: a linear transformation that connects the activation of fc1 to tensor of lenth vocab_size\n",
        "    self.fc2 = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x).view(1, -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M1gYhtWBx88h"
      },
      "source": [
        "## Train the CBOW model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LuWSX_ZGnPIG"
      },
      "source": [
        "Now that your model class is written, you must create an instance of the model and train it using the loss function `torch.nn.CrossEntropyLoss` on the output of `fc2` and `y` (the labels).\n",
        "\n",
        "Train your CBOW model for 300 epochs with `embed_dim`= 100, `window_size`=2, and `hidden_dim`=30. \n",
        "- Do not split the data into training and test sets (we will not be evaluating the performance of this model). \n",
        "- Use the SGD optimizer with learning rate = 0.001.\n",
        "- Append the loss at every epoch to a list (return the list if you use a function to fit your model), so that we can plot it later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35ybUlZs14Ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b1c56c1-3b08-478e-dc7d-b9b6b876319a"
      },
      "source": [
        "# Parameters\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 100\n",
        "WINDOW_SIZE = 2\n",
        "HIDDEN_DIM = 30\n",
        "N_EPOCHS = 300\n",
        "\n",
        "# Train your CBOW model here\n",
        "# get the train data\n",
        "review_list = []\n",
        "for i in cleaned_text:\n",
        "  review_list.extend(make_cbow_data(i, WINDOW_SIZE))\n",
        "review_list = list(filter(lambda x: x !=[], review_list))\n",
        "\n",
        "# print(review_list)\n",
        "model = CBOW(VOCAB_SIZE, EMBED_DIM, WINDOW_SIZE, HIDDEN_DIM)\n",
        "model = model.cuda()\n",
        "# set the optimizer \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
        "# loss function: use cross entry loss\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "loss_list = []\n",
        "# train CBOW model\n",
        "model.train() # put model in training mode\n",
        "for epoch in range(N_EPOCHS):\n",
        "  running_loss = 0.0\n",
        "  for x, y in review_list:\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(x)\n",
        "    loss = loss_fn(pred, y)\n",
        "    loss.backward()\n",
        "    running_loss += loss.item()\n",
        "    optimizer.step()\n",
        "  if epoch%5 == 0 or epoch == 299: \n",
        "    print(f\"Epoch {epoch} loss = {running_loss/len(review_list)}\")\n",
        "  loss_list.append(running_loss / len(review_list))"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss = 6.220800905666946\n",
            "Epoch 5 loss = 5.770757470407245\n",
            "Epoch 10 loss = 5.1379095190170085\n",
            "Epoch 15 loss = 4.503150110258922\n",
            "Epoch 20 loss = 3.9800749572566856\n",
            "Epoch 25 loss = 3.5118585819652495\n",
            "Epoch 30 loss = 3.0840301542012867\n",
            "Epoch 35 loss = 2.6748533160335772\n",
            "Epoch 40 loss = 2.281948311722119\n",
            "Epoch 45 loss = 1.9110925845901467\n",
            "Epoch 50 loss = 1.5699156191515533\n",
            "Epoch 55 loss = 1.2652156002277073\n",
            "Epoch 60 loss = 1.0027008549065328\n",
            "Epoch 65 loss = 0.7838657518729767\n",
            "Epoch 70 loss = 0.6082176131266895\n",
            "Epoch 75 loss = 0.4716165820344242\n",
            "Epoch 80 loss = 0.3680669471377958\n",
            "Epoch 85 loss = 0.2906250592326693\n",
            "Epoch 90 loss = 0.23341106624518249\n",
            "Epoch 95 loss = 0.19114343195400832\n",
            "Epoch 100 loss = 0.15970211539190557\n",
            "Epoch 105 loss = 0.13596294822522825\n",
            "Epoch 110 loss = 0.11771060240746961\n",
            "Epoch 115 loss = 0.10335753470614974\n",
            "Epoch 120 loss = 0.0918757309708163\n",
            "Epoch 125 loss = 0.08251713076813968\n",
            "Epoch 130 loss = 0.07476554972633033\n",
            "Epoch 135 loss = 0.06824655376961468\n",
            "Epoch 140 loss = 0.0627098834532331\n",
            "Epoch 145 loss = 0.05794681305339751\n",
            "Epoch 150 loss = 0.05381372601989474\n",
            "Epoch 155 loss = 0.05019237665683696\n",
            "Epoch 160 loss = 0.04700013822467685\n",
            "Epoch 165 loss = 0.04416591026453526\n",
            "Epoch 170 loss = 0.04163123948435904\n",
            "Epoch 175 loss = 0.039357817367848456\n",
            "Epoch 180 loss = 0.03730417962945765\n",
            "Epoch 185 loss = 0.03544238913820899\n",
            "Epoch 190 loss = 0.033747606433341265\n",
            "Epoch 195 loss = 0.03219721682181507\n",
            "Epoch 200 loss = 0.0307748774716971\n",
            "Epoch 205 loss = 0.02946631302628085\n",
            "Epoch 210 loss = 0.0282581607087406\n",
            "Epoch 215 loss = 0.02713926550537835\n",
            "Epoch 220 loss = 0.026100454854752653\n",
            "Epoch 225 loss = 0.025134137622668876\n",
            "Epoch 230 loss = 0.02423294798580357\n",
            "Epoch 235 loss = 0.023390383103981414\n",
            "Epoch 240 loss = 0.022602084132927133\n",
            "Epoch 245 loss = 0.021861193970089326\n",
            "Epoch 250 loss = 0.021165307882459166\n",
            "Epoch 255 loss = 0.020510105286097986\n",
            "Epoch 260 loss = 0.019892152670056822\n",
            "Epoch 265 loss = 0.019307869150238265\n",
            "Epoch 270 loss = 0.01875537146785107\n",
            "Epoch 275 loss = 0.018231904878658844\n",
            "Epoch 280 loss = 0.017735128416880457\n",
            "Epoch 285 loss = 0.017263588189724472\n",
            "Epoch 290 loss = 0.016814879464182066\n",
            "Epoch 295 loss = 0.01638781191866961\n",
            "Epoch 299 loss = 0.016060408298894872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lE_aC7oJx2xC"
      },
      "source": [
        "## Plot losses by epochs (x-axis: epoch, y-axis: loss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdAEHlvX14Ur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e9e25dca-2d7a-4fb7-f213-d8b1a83b9aaf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_list)\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.show()"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GWj-u7YJFxF7"
      },
      "source": [
        "## Print five most similar words with the word \"delicious\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vvxTFjD7GCDc"
      },
      "source": [
        "The whole point of traiing an embedding model is to get an embedding vector for each word. The idea is that the vector somehow captures the meaning of the word. This is useful because data scientists often face scenarios where they must derive meaning from unstructured text data.\n",
        "\n",
        "Once your model has been trained, you can access the embedding vectors through `model.embedding.weight.data`. You can convert these vectors to a numpy matrix or numpy arrays if needed.\n",
        "\n",
        "Find the five most similar words with the word \"delicious\" by calculating cosine similarity between the embedding vector of \"delicious\" and the embedding vectors of all other words in the vocabulary. \n",
        "\n",
        "Hint: cosine similarity is a common metric so you should be able to find one that you can use in an existing library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS5bfhBW14U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Insert your code here\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "dd = model.embedding.weight.data\n",
        "dd = dd.cpu().detach().numpy()\n",
        "dd_p = pd.DataFrame(dd)\n",
        "result = pd.DataFrame(cosine_similarity(dd_p, dd_p))\n",
        "delicious_index = word_to_index[\"delicious\"]\n",
        "top_6 = result[delicious_index].sort_values(ascending = False).iloc[0:6].index\n",
        "# list(word_to_index.keys())[371]\n",
        "# find the "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bnhuEbomCnw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56a7f27a-8266-4e64-cfc8-9db034292660"
      },
      "source": [
        "list(list(word_to_index.keys())[i] for i in top_6)"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['delicious', 'delight', 'lot', 'sever', 'after', 'wave']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Koa4azpTodz9",
        "colab_type": "text"
      },
      "source": [
        "The five most similar words to `delicious` are `delight`, `lot`, `server`, `after`, `wave`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrmuZEE5ETn",
        "colab_type": "text"
      },
      "source": [
        "While the model learns embedding vectors (that best predict the focal word from its contexts), the vectors that it learns don't seem to truly capture the meaning of words. However, this is mainly due to the small size of our training data. Google trained a word2vec model based on large-scale data (about 100 billion words), and this model captures similarity between words well. You can find the pretrained model at https://code.google.com/archive/p/word2vec/."
      ]
    }
  ]
}